{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLBanjNZqQc1"
   },
   "source": [
    "# Energy Consumption Forecasting with Darts\n",
    "\n",
    "This notebook implements a comprehensive time series forecasting project for predicting energy consumption in the PJM East region using multiple models.\n",
    "\n",
    "**Reference Documentation:** [darts.example.md](darts.example.md)\n",
    "\n",
    "**Citations:**\n",
    "- Darts Library: https://unit8co.github.io/darts/\n",
    "- Herzen et al. (2022). \"Darts: User-Friendly Modern Machine Learning for Time Series\" JMLR 23(124):1‚àí6\n",
    "- Dataset: https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption\n",
    "\n",
    "**Notebook Flow:**\n",
    "1. Setup and Imports\n",
    "2. Data Ingestion\n",
    "3. Exploratory Data Analysis\n",
    "4. Feature Engineering\n",
    "5. Model Comparison (Prophet, N-BEATS, LSTM)\n",
    "6. Hyperparameter Tuning\n",
    "7. Final Evaluation and Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmxUqJULqQc4"
   },
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Darts core imports.\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.utils.statistics import check_seasonality\n",
    "from darts.metrics import mape, rmse, mae, smape\n",
    "from darts.utils.utils import ModelMode\n",
    "\n",
    "# Darts models.\n",
    "from darts.models import (\n",
    "    Prophet,\n",
    "    NBEATSModel,\n",
    "    RNNModel,\n",
    "    ExponentialSmoothing,\n",
    "    NaiveSeasonal\n",
    ")\n",
    "\n",
    "# Local utility functions.\n",
    "import darts_utils as utils\n",
    "\n",
    "# Configure logging.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Set plotting style.\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4sc-Qy3qQc5"
   },
   "source": [
    "## 2. Data Ingestion\n",
    "\n",
    "Load the PJME Hourly Energy Consumption dataset and prepare it for time series analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PJME hourly energy consumption data using utility function.\n",
    "data_path = 'PJME_hourly.csv'\n",
    "df = utils.load_energy_data(data_path)\n",
    "\n",
    "# Display basic info.\n",
    "print(f\"üìä Dataset Shape: {df.shape}\")\n",
    "print(f\"üìÖ Date Range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"üìà Total Hours: {len(df):,}\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing timestamps with interpolation.\n",
    "df = utils.handle_missing_timestamps(df)\n",
    "\n",
    "# Display statistics.\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Darts TimeSeries object.\n",
    "series = utils.create_darts_series(df)\n",
    "\n",
    "# Use last 3 years of data for faster training.\n",
    "YEARS_OF_DATA = 3\n",
    "series_subset = series[-24*365*YEARS_OF_DATA:]\n",
    "\n",
    "print(f\"‚úÖ TimeSeries created!\")\n",
    "print(f\"üìä Using last {YEARS_OF_DATA} years: {len(series_subset)} hours\")\n",
    "print(f\"üìÖ Start: {series_subset.start_time()}\")\n",
    "print(f\"üìÖ End: {series_subset.end_time()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2U0ys48JqQc5"
   },
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Analyze the time series for patterns, seasonality, and distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series.\n",
    "utils.plot_time_series(\n",
    "    series_subset,\n",
    "    title='PJME Energy Consumption (Last 3 Years)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze seasonality patterns using utility function.\n",
    "utils.plot_seasonality_analysis(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for seasonality using statistical tests.\n",
    "print(\"üîç Seasonality Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "is_daily, _ = check_seasonality(series_subset, m=24, max_lag=48)\n",
    "print(f\"Daily (24h) seasonality detected: {is_daily}\")\n",
    "\n",
    "is_weekly, _ = check_seasonality(series_subset, m=168, max_lag=336)\n",
    "print(f\"Weekly (168h) seasonality detected: {is_weekly}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzzYpZ67qQc6"
   },
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Create temporal features, lag values, and rolling statistics to improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features.\n",
    "df_features = utils.create_temporal_features(df)\n",
    "print(\"‚úÖ Temporal features created!\")\n",
    "df_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag features.\n",
    "df_features = utils.add_lag_features(df_features)\n",
    "print(\"‚úÖ Lag features created!\")\n",
    "\n",
    "# Add rolling features.\n",
    "df_features = utils.add_rolling_features(df_features)\n",
    "print(\"‚úÖ Rolling features created!\")\n",
    "\n",
    "# Display all features.\n",
    "print(f\"\\nüìä Total features: {len(df_features.columns)}\")\n",
    "print(f\"Columns: {df_features.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis.\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "numeric_cols = df_features.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_features[numeric_cols].dropna().corr()\n",
    "\n",
    "# Plot correlation with target.\n",
    "target_corr = corr_matrix['energy_consumption'].drop('energy_consumption').sort_values()\n",
    "colors = ['red' if x < 0 else 'green' for x in target_corr.values]\n",
    "target_corr.plot(kind='barh', color=colors, ax=ax)\n",
    "ax.set_title('Feature Correlation with Energy Consumption', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Correlation Coefficient')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t0RMMRqqQc7"
   },
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Compare multiple forecasting models: Naive Seasonal, Exponential Smoothing, Prophet, N-BEATS, and LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure train/test split.\n",
    "TEST_SIZE = 24 * 30  # 30 days for testing.\n",
    "FORECAST_HORIZON = 24 * 7  # 7 days forecast.\n",
    "\n",
    "# Split data using utility function.\n",
    "train, test = utils.train_test_split_series(series_subset, test_size=TEST_SIZE)\n",
    "\n",
    "# Scale data for neural network models.\n",
    "train_scaled, test_scaled, scaler = utils.scale_series(train, test)\n",
    "\n",
    "print(f\"üìä Training set: {len(train)} hours ({len(train)/24:.0f} days)\")\n",
    "print(f\"üìä Test set: {len(test)} hours ({len(test)/24:.0f} days)\")\n",
    "print(f\"üéØ Forecast horizon: {FORECAST_HORIZON} hours ({FORECAST_HORIZON/24:.0f} days)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train/test split.\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "train.plot(ax=ax, label='Training Data', color='blue')\n",
    "test.plot(ax=ax, label='Test Data', color='orange')\n",
    "ax.set_title('Train/Test Split', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store model results.\n",
    "model_results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYPaiYDEqQc7"
   },
   "source": [
    "### 5.1 Naive Seasonal Model (Baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qRZfatmqQc7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Seasonal model with weekly seasonality.\n",
    "print(\"üîÑ Training Naive Seasonal Model...\")\n",
    "naive_model = NaiveSeasonal(K=168)\n",
    "naive_model.fit(train)\n",
    "naive_pred = naive_model.predict(len(test))\n",
    "\n",
    "# Evaluate using utility function.\n",
    "metrics = utils.evaluate_forecast(test, naive_pred, \"Naive Seasonal\")\n",
    "model_results['Naive Seasonal'] = {'predictions': naive_pred, **metrics}\n",
    "print(\"‚úÖ Naive Seasonal Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSA6nb9bqQc8"
   },
   "source": [
    "### 5.2 Exponential Smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Exponential Smoothing with daily seasonality.\n",
    "print(\"üîÑ Training Exponential Smoothing Model...\")\n",
    "exp_model = ExponentialSmoothing(seasonal_periods=24, trend=None, seasonal=ModelMode.ADDITIVE)\n",
    "exp_model.fit(train)\n",
    "exp_pred = exp_model.predict(len(test))\n",
    "\n",
    "# Evaluate.\n",
    "metrics = utils.evaluate_forecast(test, exp_pred, \"Exponential Smoothing\")\n",
    "model_results['Exponential Smoothing'] = {'predictions': exp_pred, **metrics}\n",
    "print(\"‚úÖ Exponential Smoothing Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49qlCSE8qQc8"
   },
   "source": [
    "### 5.3 Prophet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Prophet with multiple seasonalities.\n",
    "print(\"üîÑ Training Prophet Model...\")\n",
    "print(\"   (This may take a few minutes...)\")\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    ")\n",
    "prophet_model.fit(train)\n",
    "prophet_pred = prophet_model.predict(len(test))\n",
    "\n",
    "# Evaluate.\n",
    "metrics = utils.evaluate_forecast(test, prophet_pred, \"Prophet\")\n",
    "model_results['Prophet'] = {'predictions': prophet_pred, **metrics}\n",
    "print(\"‚úÖ Prophet Model trained!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "9JgUBSBPqQc8"
   },
   "source": [
    "### 5.4 N-BEATS Model (Deep Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train N-BEATS model.\n",
    "print(\"üîÑ Training N-BEATS Model...\")\n",
    "print(\"   (This may take several minutes...)\")\n",
    "nbeats_model = NBEATSModel(\n",
    "    input_chunk_length=168,\n",
    "    output_chunk_length=24,\n",
    "    generic_architecture=True,\n",
    "    num_stacks=10,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    batch_size=1024,\n",
    "    layer_widths=256,\n",
    "    n_epochs=50,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"enable_progress_bar\": True, \"accelerator\": \"auto\"}\n",
    ")\n",
    "nbeats_model.fit(train_scaled, verbose=True)\n",
    "\n",
    "# Predict and inverse transform.\n",
    "nbeats_pred_scaled = nbeats_model.predict(len(test))\n",
    "nbeats_pred = scaler.inverse_transform(nbeats_pred_scaled)\n",
    "\n",
    "# Evaluate.\n",
    "metrics = utils.evaluate_forecast(test, nbeats_pred, \"N-BEATS\")\n",
    "model_results['N-BEATS'] = {'predictions': nbeats_pred, **metrics}\n",
    "print(\"‚úÖ N-BEATS Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK6dXGaxqQc8"
   },
   "source": [
    "### 5.5 LSTM Model (Deep Learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM model.\n",
    "print(\"üîÑ Training LSTM Model...\")\n",
    "print(\"   (This may take several minutes...)\")\n",
    "lstm_model = RNNModel(\n",
    "    model='LSTM',\n",
    "    input_chunk_length=168,\n",
    "    output_chunk_length=24,\n",
    "    training_length=192,\n",
    "    hidden_dim=64,\n",
    "    batch_size=1024,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=50,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"enable_progress_bar\": True, \"accelerator\": \"auto\"}\n",
    ")\n",
    "lstm_model.fit(train_scaled, verbose=True)\n",
    "\n",
    "# Predict and inverse transform.\n",
    "lstm_pred_scaled = lstm_model.predict(len(test))\n",
    "lstm_pred = scaler.inverse_transform(lstm_pred_scaled)\n",
    "\n",
    "# Evaluate.\n",
    "metrics = utils.evaluate_forecast(test, lstm_pred, \"LSTM\")\n",
    "model_results['LSTM'] = {'predictions': lstm_pred, **metrics}\n",
    "print(\"‚úÖ LSTM Model trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison summary.\n",
    "summary_df = utils.compare_models(model_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "best_model_name = summary_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OD5lgqKqQc9"
   },
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "Optimize N-BEATS model using grid search and cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for N-BEATS.\n",
    "param_grid = {\n",
    "    'input_chunk_length': [168],\n",
    "    'output_chunk_length': [24],\n",
    "    'num_stacks': [5, 10],\n",
    "    'num_layers': [2, 4],\n",
    "    'layer_widths': [128, 256]\n",
    "}\n",
    "\n",
    "# Create validation set from training data.\n",
    "VAL_SIZE = 24 * 7  # 1 week.\n",
    "train_tune = train_scaled[:-VAL_SIZE]\n",
    "val_tune = train_scaled[-VAL_SIZE:]\n",
    "\n",
    "print(f\"üìä Tuning train size: {len(train_tune)} hours\")\n",
    "print(f\"üìä Validation size: {len(val_tune)} hours\")\n",
    "print(f\"üìä Total parameter combinations: {len(list(ParameterGrid(param_grid)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search.\n",
    "print(\"üîÑ Running Grid Search...\")\n",
    "print(\"   (This may take a while...)\\n\")\n",
    "\n",
    "tuning_results = []\n",
    "for params in tqdm(list(ParameterGrid(param_grid))):\n",
    "    try:\n",
    "        model = NBEATSModel(\n",
    "            input_chunk_length=params['input_chunk_length'],\n",
    "            output_chunk_length=params['output_chunk_length'],\n",
    "            generic_architecture=True,\n",
    "            num_stacks=params['num_stacks'],\n",
    "            num_blocks=1,\n",
    "            num_layers=params['num_layers'],\n",
    "            layer_widths=params['layer_widths'],\n",
    "            n_epochs=20,\n",
    "            batch_size=1024,\n",
    "            random_state=42,\n",
    "            pl_trainer_kwargs={\"enable_progress_bar\": False, \"accelerator\": \"auto\"}\n",
    "        )\n",
    "        model.fit(train_tune, verbose=False)\n",
    "        pred = model.predict(len(val_tune))\n",
    "        score = mape(val_tune, pred)\n",
    "        params['mape'] = score\n",
    "        tuning_results.append(params)\n",
    "        print(f\"   Params: {params} -> MAPE: {score:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error with params {params}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n‚úÖ Grid Search Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best parameters.\n",
    "tuning_df = pd.DataFrame(tuning_results).sort_values('mape')\n",
    "print(\"üìä Hyperparameter Tuning Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(tuning_df.to_string(index=False))\n",
    "\n",
    "best_params = tuning_df.iloc[0].to_dict()\n",
    "del best_params['mape']\n",
    "print(f\"\\nüèÜ Best Parameters:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"   {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train optimized N-BEATS model with best parameters.\n",
    "print(\"\\nüîÑ Training Optimized N-BEATS Model...\")\n",
    "optimized_nbeats = NBEATSModel(\n",
    "    input_chunk_length=int(best_params['input_chunk_length']),\n",
    "    output_chunk_length=int(best_params['output_chunk_length']),\n",
    "    generic_architecture=True,\n",
    "    num_stacks=int(best_params['num_stacks']),\n",
    "    num_blocks=1,\n",
    "    num_layers=int(best_params['num_layers']),\n",
    "    layer_widths=int(best_params['layer_widths']),\n",
    "    n_epochs=200,\n",
    "    batch_size=1024,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"enable_progress_bar\": True, \"accelerator\": \"auto\"}\n",
    ")\n",
    "optimized_nbeats.fit(train_scaled, verbose=True)\n",
    "\n",
    "# Predict and evaluate.\n",
    "optimized_pred_scaled = optimized_nbeats.predict(len(test))\n",
    "optimized_pred = scaler.inverse_transform(optimized_pred_scaled)\n",
    "metrics = utils.evaluate_forecast(test, optimized_pred, \"Optimized N-BEATS\")\n",
    "model_results['Optimized N-BEATS'] = {'predictions': optimized_pred, **metrics}\n",
    "print(\"\\n‚úÖ Optimized N-BEATS Model trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14VP3qI2qQc9"
   },
   "source": [
    "## 7. Final Evaluation and Visualization\n",
    "\n",
    "Analyze model performance and visualize predictions across different time windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model comparison summary.\n",
    "final_df = utils.compare_models(model_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "best_model_name = final_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   MAPE: {final_df.iloc[0]['MAPE (%)']}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dict = {name: results['predictions'] for name, results in model_results.items()}\n",
    "\n",
    "# Corrected plotting logic, inlining the functionality of plot_predictions_vs_actual\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Plot actual data\n",
    "test.plot(ax=ax, label='Actual', color='black', linewidth=2)\n",
    "\n",
    "# Plot predictions for each model\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(predictions_dict)))\n",
    "for (model_name, pred), color in zip(predictions_dict.items(), colors):\n",
    "    # Convert numpy array color to a tuple for darts plot function\n",
    "    pred.plot(ax=ax, label=model_name, color=tuple(color), linewidth=1.5, alpha=0.8)\n",
    "\n",
    "ax.set_title('All Model Predictions vs Actual', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "ax.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed view: Best model prediction vs actual by week.\n",
    "best_predictions = model_results[best_model_name]['predictions']\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
    "weeks = [(0, 168, 'Week 1'), (168, 336, 'Week 2'), (336, 504, 'Week 3'), (504, 672, 'Week 4')]\n",
    "\n",
    "for idx, (start, end, week_name) in enumerate(weeks):\n",
    "    ax = axes[idx]\n",
    "    actual_week = test[start:end]\n",
    "    pred_week = best_predictions[start:end]\n",
    "    actual_week.plot(ax=ax, label='Actual', color='black', linewidth=2)\n",
    "    pred_week.plot(ax=ax, label=f'{best_model_name}', color='green', linewidth=2, alpha=0.8)\n",
    "    weekly_mape = mape(actual_week, pred_week)\n",
    "    ax.set_title(f'{week_name} - MAPE: {weekly_mape:.2f}%', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Energy (MW)')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.fill_between(\n",
    "        actual_week.time_index,\n",
    "        actual_week.univariate_values().flatten(),\n",
    "        pred_week.univariate_values().flatten(),\n",
    "        alpha=0.3, color='red'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_model_weekly.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"üìÅ Saved: best_model_weekly.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis using utility function.\n",
    "utils.plot_error_analysis(test, best_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate future forecast.\n",
    "print(\"üîÆ Generating Future Forecast...\")\n",
    "\n",
    "# Train on all available data.\n",
    "full_train_scaled = scaler.fit_transform(series_subset)\n",
    "future_model = NBEATSModel(\n",
    "    input_chunk_length=int(best_params.get('input_chunk_length', 168)),\n",
    "    output_chunk_length=int(best_params.get('output_chunk_length', 24)),\n",
    "    generic_architecture=True,\n",
    "    num_stacks=int(best_params.get('num_stacks', 10)),\n",
    "    num_blocks=1,\n",
    "    num_layers=int(best_params.get('num_layers', 4)),\n",
    "    layer_widths=int(best_params.get('layer_widths', 256)),\n",
    "    n_epochs=50,\n",
    "    batch_size=1024,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"enable_progress_bar\": True, \"accelerator\": \"auto\"}\n",
    ")\n",
    "future_model.fit(full_train_scaled, verbose=True)\n",
    "\n",
    "# Predict next 7 days.\n",
    "FUTURE_HORIZON = 24 * 7\n",
    "future_pred_scaled = future_model.predict(FUTURE_HORIZON)\n",
    "future_pred = scaler.inverse_transform(future_pred_scaled)\n",
    "print(f\"\\n‚úÖ Generated {FUTURE_HORIZON} hour forecast!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot future forecast.\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "historical = series_subset[-24*14:]\n",
    "historical.plot(ax=ax, label='Historical Data', color='blue', linewidth=2)\n",
    "future_pred.plot(ax=ax, label='7-Day Forecast', color='red', linewidth=2)\n",
    "ax.set_title('7-Day Energy Consumption Forecast', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.axvline(x=series_subset.end_time(), color='green', linestyle='--', linewidth=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig('future_forecast.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"üìÅ Saved: future_forecast.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uemfLbIcqQc-"
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Seasonality**: The PJME energy data shows strong daily, weekly, and yearly seasonal patterns.\n",
    "2. **Model Performance**: Deep learning models (N-BEATS, LSTM) generally outperform traditional statistical methods.\n",
    "3. **Best Model**: The optimized N-BEATS model achieved the best performance after hyperparameter tuning.\n",
    "4. **Feature Importance**: Temporal features (hour of day, day of week) are highly correlated with energy consumption.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. For production deployment, consider ensemble methods combining multiple models.\n",
    "2. Incorporate external features like weather data for improved accuracy.\n",
    "3. Regularly retrain models as consumption patterns may shift over time.\n",
    "4. Monitor model performance and implement automated retraining pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results.\n",
    "import json\n",
    "\n",
    "final_df.to_csv('model_results.csv', index=False)\n",
    "print(\"üìÅ Saved: model_results.csv\")\n",
    "\n",
    "with open('best_params.json', 'w') as f:\n",
    "    json.dump({k: int(v) if isinstance(v, (int, np.integer)) else v\n",
    "               for k, v in best_params.items()}, f, indent=2)\n",
    "print(\"üìÅ Saved: best_params.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ ENERGY FORECASTING PROJECT COMPLETE!\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
